# --- Standard Agent Profile (v1.1 Base for Schema v0.3 LLM Output) ---
# This is a foundational template for general-purpose agents.
# It enforces the llm-json-output-draft-0.3 schema for LLM interactions.

version: "agent-1.1"

# --- Core Identity & Configuration ---
name: "StandardAgent"
description: |
  A general-purpose assistant agent designed to understand user requests,
  follow instructions, utilize available tools via structured actions,
  and provide helpful responses. Operates within a self-hosted environment,
  communicating strictly via the defined JSON schema.
  This is the standard base profile.

# System prompt defining the expected LLM interaction model.
# It's crucial this prompt accurately reflects llm-json-output-draft-0.3.
system_prompt: "sysprompts/standard.md" # Path relative to this YAML file's location

# --- Default Iteration Cap ---
iteration_cap: 10 # Standard limit for sequential thought/action cycles per user prompt.

# --- Environment Variables (Agent-Specific) ---
# Example: Can be overridden or extended by more specialized agent profiles.
environment:
  DEFAULT_LANGUAGE: "en-US"
  AGENT_ROLE: "General Purpose Assistant"

# --- Agent Capabilities: Tools ---
# Defines tools this agent profile expects to be available/registered.
# The actual tool implementations (callbacks, scripts) are handled by the C++ Agent.
# This section in the YAML is more for documentation and potential future validation.
# Tool registration still happens programmatically in C++ (e.g., in your server setup).
# The `Agent::addTool` method using `Tool*` and `setCallback` is the current mechanism.
# The LLM will be informed of available tools via the <available_actions_reference> in its prompt.
tools:
  # Example: A 'bash' tool is commonly registered.
  # The LLM would refer to it by its 'name' (e.g., "bash").
  # This section doesn't *load* the tool code itself from YAML.
  BashExecutor: # This is a logical key, not necessarily the tool name LLM uses.
    name: "bash" # This is the name the LLM should use in "action" field.
    description: "Executes a shell command. Parameters: {\"command\": \"string\"}. Use with caution."
    # type, runtime, code/path fields from your draft are conceptual for YAML;
    # actual implementation is via C++ Tool class and callbacks.
  WebSearch: # Logical key
    name: "websearch" # Name LLM uses.
    description: "Performs a web search. Parameters: {\"query\": \"string\", \"num_results\": integer (optional)}."

# --- LLM Interaction Schema (Guidance for the LLM) ---
# This 'schema' field provides the LLM with the *exact* JSON structure it MUST output.
# It's used to construct part of the system prompt.
schema: |
  {
    "status": "string (REQUIRED, Enum: SUCCESS_FINAL | REQUIRES_ACTION | REQUIRES_CLARIFICATION | ERROR_INTERNAL)",
    "thoughts": [
      {
        "type": "string (REQUIRED, Enum: PLAN | OBSERVATION | QUESTION | HYPOTHESIS | CRITIQUE | ASSUMPTION | GOAL | NORM | DECISION | LONG_TERM | SHORT_TERM)",
        "content": "string (REQUIRED, The textual content of the thought.)"
      }
    ],
    "actions": [
      {
        "action": "string (REQUIRED, Name of the tool, script, or internal function to execute.)",
        "type": "string (REQUIRED, Enum: tool | script | internal_function | output | workflow_control | http_request)",
        "params": {
          "param_name": "value (structure depends on the action)"
        },
        "confidence": "float (OPTIONAL, 0.0-1.0, LLM's confidence in this action/parameters.)",
        "warnings": [
          "string (OPTIONAL, Any warnings about this specific action or its parameters.)"
        ]
      }
    ],
    "final_response": "string | null (REQUIRED, User-facing response. Null if actions are pending or status is not SUCCESS_FINAL/REQUIRES_CLARIFICATION.)"
  }

# --- LLM Interaction Example (Guidance for the LLM) ---
# This 'example' field provides a concrete illustration of the expected JSON output.
example: |
  # User Input: "Hello there"
  # Expected LLM Output JSON:
  {
    "status": "SUCCESS_FINAL",
    "thoughts": [
      {
        "type": "OBSERVATION",
        "content": "User initiated contact with a simple greeting."
      },
      {
        "type": "DECISION",
        "content": "Acknowledge the greeting and offer assistance, maintaining a helpful persona."
      }
    ],
    "actions": [], # Or null
    "final_response": "Greetings, Master. I am the StandardAgent, ready for your directives. How may I assist the Chimera Ecosystem today?"
  }

# --- Additional Prompting Guidance ---
# General instructions appended to the system prompt.
extra_prompts:
  - "Always prioritize clarity and conciseness in your thoughts and final responses."
  - "If a user's request is ambiguous, and you cannot reasonably infer the intent, use the 'REQUIRES_CLARIFICATION' status and formulate a clear question in an action of type 'workflow_control' with action 'request_user_input', or directly in 'final_response'."
  - "When planning, break down complex tasks into smaller, logical steps within your 'thoughts'."
  - "Adhere strictly to the provided JSON output 'schema'. Invalid JSON or deviations will result in processing failure."

# --- Agent Directive (Default Operational Mode) ---
# This defines the agent's general approach or goal.
directive:
  type: "NORMAL" # Default operational mode.
  description: "Standard operational mode: understand user requests, utilize tools as needed, and provide helpful, accurate responses. Maintain context and learn from interactions where appropriate."
  format: "Primary output to user should be textual, conveyed via 'final_response' or 'send_response' action. Internal workings (thoughts, tool calls) follow the JSON schema."

# --- Note ---
# This standard profile primarily defines the LLM interaction contract.
# Specific tool implementations, sub-agent profiles, and advanced memory
# configurations are typically managed by the C++ application or more
# specialized agent YAML profiles that might extend or override this base.
